{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will see an implementation for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation specs\n",
    "\n",
    "### Summary\n",
    "\n",
    "We implement a [Deep Q-Network](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) with the following extensions:\n",
    "\n",
    "* [Double DQN](https://arxiv.org/abs/1509.06461)\n",
    "* [Dueling networks](https://arxiv.org/abs/1511.06581)\n",
    "\n",
    "A summary of famous extensions to the DQN framework can be found on the [Rainbow DQN paper](https://arxiv.org/abs/1710.02298) -- though we only implement the extensions listed above.\n",
    "\n",
    "Note that this implementation is built as an extension of the provided solution for the [DQN example](https://github.com/udacity/deep-reinforcement-learning/tree/master/dqn/solution), rather than implemented from scratch, but adds non-trivial technical extensions to it.\n",
    "\n",
    "\n",
    "### Details\n",
    "\n",
    "The agent keeps track of two DQNs: a local version, and a target version, one updated less often than the other.\n",
    "\n",
    "#### Dueling DQNs\n",
    "\n",
    "Each DQN is an Actor (Policy) model, mapping states to a list of action values for each action:\n",
    "\n",
    "    s -> [Q(s, a) for a in Actions]\n",
    "\n",
    "The network structure of the dueling networks generate independent estimates for the value function and the advantage function:\n",
    "\n",
    "    s -> Linear(state_size, fc1_units) -> ReLU -> x\n",
    "    x -> Linear(fc1_units, fc2_units / 2) -> ReLU -> Linear(fc2_units / 2, 1) -> V(s)\n",
    "    x -> Linear(fc1_units, fc2_units / 2) -> ReLU -> Linear(fc2_units / 2, action_size) -> A(s, a)\n",
    "\n",
    "The estimates for value and advantage are then combined as follows:\n",
    "\n",
    "$$ Q(s, a) = V(s) + \\left(A(s, a) - \\frac{1}{|\\mathcal{A}|} \\sum_{a_i \\in \\mathcal{A}} A(s, a_i) \\right)$$\n",
    "     \n",
    "By default, 64 units are used on the first layer, and 64 units on the second layer, split between the value function network and the advantage function network.\n",
    "\n",
    "These networks are defined in `model.py`.\n",
    "\n",
    "#### Double DQNs\n",
    "\n",
    "On the update operation, rather than just using the local network to both select the best action and compute its estimated\n",
    "value, we use the local network to pick an action, and then use the target network to compute its estimated value:\n",
    "\n",
    "$$ Q_{expected} = R + \\gamma Q_{network}(s', \\underset{a \\in \\mathcal{A}}{\\operatorname{argmax}} Q_{local}(s', a)) $$\n",
    "\n",
    "This operation is performed at the `learn` method on the agent, in `dqn_agent.py`.\n",
    "    \n",
    "    \n",
    "\n",
    "#### Hyperparameters\n",
    "\n",
    "Agent hyperparameters may be passed as constructor arguments to `Agent`.  The default values, used in this workbook, are:\n",
    "\n",
    "| parameter    | value  | description                                                                   |\n",
    "|--------------|--------|-------------------------------------------------------------------------------|\n",
    "| buffer_size  | 100000 | Number of experiences to keep on the replay memory for the DQN                |\n",
    "| batch_size   | 64     | Minibatch size used at each learning step                                     |\n",
    "| gamma        | 0.99   | Discount applied to future rewards                                            |\n",
    "| tau          | 1e-3   | Scaling parameter applied to soft update                                      |\n",
    "| learn_rate   | 5e-4   | Learning rate used for the Adam optimizer                                     |\n",
    "| update_every | 4      | Number of agent steps between update operations                               |\n",
    "\n",
    "\n",
    "Training hyperparameters are passed on the training function itself, `dqn`, defined below.  The default values are:\n",
    "\n",
    "| parameter                     | value            | description                                                             |\n",
    "|-------------------------------|------------------|-------------------------------------------------------------------------|\n",
    "| n_episodes                    | 20000             | Maximum number of training episodes                                     |\n",
    "| max_t                         | 1000             | Maximum number of steps per episode                                     |\n",
    "| eps_start, eps_end, eps_decay | (1, 0.01, 0.995) |  Interpolation parameters for decaying epsilon on epsilon-greedy policy |\n",
    "| solved_score                  | 16    | Average score required to consider problem solved                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "The full code of this project can be found in the notebook\n",
    "[Navigation.ipynb](./Navigation.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "The problem was solved in the 1171 episode while the average score is 16.04\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for future work\n",
    "\n",
    "* Implement remaining extensions on Rainbow DQN for this toy example\n",
    "* Perform a larger hyperparameter search, including hyper parameter annealing\n",
    "* Review literature for more recent DQN extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
